{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\ethan ohman\\anaconda3\\lib\\site-packages (1.26.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\ethan ohman\\anaconda3\\lib\\site-packages (0.24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in c:\\users\\ethan ohman\\anaconda3\\lib\\site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\ethan ohman\\anaconda3\\lib\\site-packages (from pandas) (1.16.4)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\users\\ethan ohman\\anaconda3\\lib\\site-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ethan ohman\\appdata\\roaming\\python\\python37\\site-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import PyPDF2 as pdf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AAPL IS DONE \n",
    "# ADBE IS DONE \n",
    "# start from BA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm done!\n"
     ]
    }
   ],
   "source": [
    "#Howdy Script User, this script follows a natural progression in which you first grab the table of contents, \n",
    "#grab data from that table of contents, grab the needed pages from a a page-range list and then analyze each er document\n",
    "#getting the needed information out of it\n",
    "\n",
    "## FOR MSFT , make sure to skip document number 8 \n",
    "# if(index == 8):\n",
    "#            continue \n",
    "\n",
    "#Data will be gathered by SMP 500 selected stock. :^) \n",
    "\n",
    "#!Important - make sure you fill up every single data dictionary space!\n",
    "\n",
    "## just enter in proper ticker and the number of equity research reports\n",
    "def get_TOC(ticker, num_docs):\n",
    "    for index in range(1, (num_docs+1)):\n",
    "        export_path = \"/Users/Ethan Ohman/Desktop/Equity Research/{0}/{0}_data_tc_txt/{0} ({1}).pdf\".format(ticker, index)\n",
    "        path = \"/Users/Ethan Ohman/Desktop/Equity Research/{0}/{0} ({1}).pdf\".format(ticker,index)\n",
    "        export_doc = pdf.PdfFileWriter()\n",
    "        doc = pdf.PdfFileReader(open((path),\"rb\"))\n",
    "        for place in range(7):  \n",
    "            page = doc.getPage(place)\n",
    "            try:\n",
    "                test = page.extractText()[1]\n",
    "                export_doc.addPage(page)\n",
    "            except:\n",
    "                pass\n",
    "        export_doc.write(open(export_path,\"wb\"))\n",
    "    print(\"I'm done!\")\n",
    "get_TOC(\"NFLX\",5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Bank': ['EVERCORE ISI', 'EVERCORE ISI', 'JEFFERIES', 'JPMORGAN', 'JPMORGAN', 'JPMORGAN', 'JPMORGAN', 'JPMORGAN', 'EVERCORE ISI', 'MORGAN STANLEY', 'EVERCORE ISI', 'JPMORGAN', 'JEFFERIES', 'EVERCORE ISI', 'JPMORGAN', 'JPMORGAN', 'JPMORGAN', 'JPMORGAN', 'EVERCORE ISI', 'EVERCORE ISI', 'JEFFERIES', 'JPMORGAN', 'JPMORGAN', 'JPMORGAN', 'JPMORGAN', 'MORGAN STANLEY', 'MORGAN STANLEY', 'JPMORGAN', 'EVERCORE ISI', 'JPMORGAN', 'JPMORGAN', 'EVERCORE ISI', 'JEFFERIES', 'JPMORGAN', 'JPMORGAN', 'JEFFERIES', 'JPMORGAN', 'EVERCORE ISI', 'JEFFERIES', 'JPMORGAN', 'EVERCORE ISI', 'JPMORGAN', 'JEFFERIES', 'JPMORGAN', 'JPMORGAN', 'JPMORGAN', 'EVERCORE ISI', 'JEFFERIES', 'JPMORGAN', 'JEFFERIES', 'JPMORGAN', 'JPMORGAN', 'EVERCORE ISI', 'JPMORGAN', 'JEFFERIES', 'EVERCORE ISI', 'JPMORGAN', 'JEFFERIES', 'JPMORGAN', 'JPMORGAN', 'JPMORGAN', 'JPMORGAN', 'EVERCORE ISI', 'JPMORGAN', 'JEFFERIES', 'JPMORGAN', 'EVERCORE ISI', 'JEFFERIES', 'JPMORGAN', 'EVERCORE ISI', 'JPMORGAN', 'JEFFERIES', 'JPMORGAN', 'JPMORGAN', 'EVERCORE ISI', 'JEFFERIES', 'MORGAN STANLEY', 'EVERCORE ISI', 'JPMORGAN', 'JEFFERIES', 'JPMORGAN', 'MORGAN STANLEY', 'JPMORGAN', 'EVERCORE ISI', 'JPMORGAN', 'JEFFERIES', 'EVERCORE ISI', 'JEFFERIES', 'MORGAN STANLEY', 'EVERCORE ISI', 'JPMORGAN', 'MORGAN STANLEY', 'JPMORGAN', 'JPMORGAN', 'MORGAN STANLEY', 'JEFFERIES', 'EVERCORE ISI', 'MORGAN STANLEY', 'EVERCORE ISI', 'JPMORGAN', 'JPMORGAN', 'JPMORGAN', 'MORGAN STANLEY', 'EVERCORE ISI', 'MORGAN STANLEY', 'JPMORGAN', 'JPMORGAN', 'MORGAN STANLEY', 'EVERCORE ISI', 'JEFFERIES', 'EVERCORE ISI', 'JPMORGAN', 'JPMORGAN', 'JEFFERIES', 'MORGAN STANLEY', 'JPMORGAN', 'MORGAN STANLEY', 'EVERCORE ISI', 'MORGAN STANLEY', 'JPMORGAN', 'JEFFERIES', 'MORGAN STANLEY', 'MORGAN STANLEY', 'JEFFERIES', 'EVERCORE ISI', 'JPMORGAN', 'JPMORGAN', 'JPMORGAN', 'MORGAN STANLEY', 'EVERCORE ISI', 'JEFFERIES', 'JPMORGAN', 'MORGAN STANLEY', 'JPMORGAN', 'JPMORGAN', 'JEFFERIES', 'JEFFERIES', 'EVERCORE ISI', 'EVERCORE ISI', 'JEFFERIES', 'MORGAN STANLEY', 'JPMORGAN', 'MORGAN STANLEY', 'JPMORGAN', 'JPMORGAN', 'MORGAN STANLEY', 'MORGAN STANLEY', 'JEFFERIES', 'EVERCORE ISI', 'JPMORGAN', 'JPMORGAN', 'MORGAN STANLEY', 'JEFFERIES', 'EVERCORE ISI', 'JPMORGAN', 'MORGAN STANLEY', 'JPMORGAN', 'JPMORGAN', 'OPPENHEIMER AND CO', 'OPPENHEIMER AND CO', 'JEFFERIES', 'EVERCORE ISI', 'OPPENHEIMER AND CO', 'MORGAN STANLEY', 'MORGAN STANLEY', 'JPMORGAN', 'JPMORGAN', 'JPMORGAN', 'JPMORGAN', 'OPPENHEIMER AND CO', 'JEFFERIES', 'EVERCORE ISI', 'OPPENHEIMER AND CO', 'MORGAN STANLEY', 'MORGAN STANLEY', 'JPMORGAN', 'JPMORGAN', 'JPMORGAN', 'OPPENHEIMER AND CO', 'EVERCORE ISI', 'JPMORGAN', 'JEFFERIES', 'OPPENHEIMER AND CO', 'JPMORGAN', 'MORGAN STANLEY', 'JPMORGAN', 'OPPENHEIMER AND CO', 'EVERCORE ISI', 'MORGAN STANLEY', 'JPMORGAN', 'OPPENHEIMER AND CO', 'JPMORGAN', 'OPPENHEIMER AND CO', 'JEFFERIES', 'JPMORGAN', 'JEFFERIES', 'JPMORGAN', 'EVERCORE ISI', 'OPPENHEIMER AND CO', 'MORGAN STANLEY', 'JPMORGAN', 'OPPENHEIMER AND CO', 'OPPENHEIMER AND CO', 'OPPENHEIMER AND CO', 'JEFFERIES', 'JPMORGAN', 'JPMORGAN', 'MORGAN STANLEY', 'OPPENHEIMER AND CO', 'JPMORGAN', 'OPPENHEIMER AND CO', 'JPMORGAN', 'OPPENHEIMER AND CO'], 'Date': ['04-Nov-2019', '01-Nov-2019', '01-Nov-2019', '01-Nov-2019', '01-Nov-2019', '16-Oct-2019', '01-Oct-2019', '30-Sep-2019', '23-Aug-2019', '22-Aug-2019', '05-Aug-2019', '03-Aug-2019', '02-Aug-2019', '02-Aug-2019', '02-Aug-2019', '16-Jul-2019', '01-Jul-2019', '18-Jun-2019', '30-May-2019', '29-Apr-2019', '26-Apr-2019', '26-Apr-2019', '26-Apr-2019', '26-Apr-2019', '05-Apr-2019', '01-Apr-2019', '01-Apr-2019', '12-Mar-2019', '07-Mar-2019', '06-Mar-2019', '26-Feb-2019', '04-Feb-2019', '04-Feb-2019', '03-Feb-2019', '01-Feb-2019', '01-Feb-2019', '09-Jan-2019', '05-Nov-2018', '05-Nov-2018', '03-Nov-2018', '02-Nov-2018', '02-Nov-2018', '02-Nov-2018', '30-Oct-2018', '08-Oct-2018', '18-Sep-2018', '30-Jul-2018', '30-Jul-2018', '27-Jul-2018', '27-Jul-2018', '27-Jul-2018', '20-Jul-2018', '30-Apr-2018', '29-Apr-2018', '27-Apr-2018', '27-Apr-2018', '27-Apr-2018', '27-Apr-2018', '23-Apr-2018', '27-Mar-2018', '21-Mar-2018', '09-Mar-2018', '08-Mar-2018', '08-Mar-2018', '02-Mar-2018', '01-Mar-2018', '05-Feb-2018', '05-Feb-2018', '03-Feb-2018', '02-Feb-2018', '02-Feb-2018', '02-Feb-2018', '23-Jan-2018', '18-Dec-2017', '30-Oct-2017', '30-Oct-2017', '30-Oct-2017', '27-Oct-2017', '27-Oct-2017', '27-Oct-2017', '27-Oct-2017', '27-Oct-2017', '17-Oct-2017', '21-Aug-2017', '18-Aug-2017', '17-Aug-2017', '31-Jul-2017', '31-Jul-2017', '31-Jul-2017', '28-Jul-2017', '28-Jul-2017', '28-Jul-2017', '28-Jul-2017', '19-Jul-2017', '02-May-2017', '01-May-2017', '01-May-2017', '30-Apr-2017', '28-Apr-2017', '28-Apr-2017', '28-Apr-2017', '17-Apr-2017', '06-Mar-2017', '02-Mar-2017', '02-Mar-2017', '01-Mar-2017', '27-Feb-2017', '02-Feb-2017', '01-Feb-2017', '01-Feb-2017', '31-Jan-2017', '31-Jan-2017', '31-Jan-2017', '31-Jan-2017', '31-Jan-2017', '25-Jan-2017', '19-Jan-2017', '18-Jan-2017', '18-Jan-2017', '17-Jan-2017', '17-Jan-2017', '01-Nov-2016', '31-Oct-2016', '31-Oct-2016', '31-Oct-2016', '28-Oct-2016', '28-Oct-2016', '20-Oct-2016', '02-Aug-2016', '01-Aug-2016', '01-Aug-2016', '29-Jul-2016', '29-Jul-2016', '29-Jul-2016', '27-Jul-2016', '18-Jul-2016', '30-Jun-2016', '12-May-2016', '02-May-2016', '02-May-2016', '02-May-2016', '29-Apr-2016', '29-Apr-2016', '29-Apr-2016', '27-Apr-2016', '04-Mar-2016', '03-Mar-2016', '03-Mar-2016', '03-Mar-2016', '02-Mar-2016', '29-Feb-2016', '04-Feb-2016', '03-Feb-2016', '03-Feb-2016', '02-Feb-2016', '02-Feb-2016', '02-Feb-2016', '01-Feb-2016', '11-Jan-2016', '30-Nov-2015', '02-Nov-2015', '02-Nov-2015', '02-Nov-2015', '02-Nov-2015', '01-Nov-2015', '30-Oct-2015', '30-Oct-2015', '29-Oct-2015', '17-Sep-2015', '08-Sep-2015', '03-Aug-2015', '03-Aug-2015', '03-Aug-2015', '03-Aug-2015', '03-Aug-2015', '31-Jul-2015', '31-Jul-2015', '29-Jul-2015', '19-May-2015', '01-May-2015', '01-May-2015', '01-May-2015', '01-May-2015', '30-Apr-2015', '30-Apr-2015', '29-Apr-2015', '12-Apr-2015', '05-Mar-2015', '05-Mar-2015', '04-Mar-2015', '03-Mar-2015', '02-Mar-2015', '02-Mar-2015', '27-Feb-2015', '03-Feb-2015', '02-Feb-2015', '02-Feb-2015', '02-Feb-2015', '02-Feb-2015', '02-Feb-2015', '30-Jan-2015', '05-Jan-2015', '05-Jan-2015', '03-Dec-2014', '03-Nov-2014', '03-Nov-2014', '31-Oct-2014', '31-Oct-2014', '31-Oct-2014', '30-Oct-2014', '17-Oct-2014', '23-Sep-2014', '02-Sep-2014'], 'Position': [], 'Text': [], 'Price Target': []}\n",
      "{1: [(6, 21), (22, 28), (29, 35), (36, 42), (43, 58), (59, 70), (71, 76), (77, 87), (88, 97), (98, 108), (109, 125), (126, 137), (138, 144), (145, 151), (152, 158), (159, 172), (173, 184), (185, 190), (191, 222), (223, 237), (238, 244), (245, 250), (251, 257), (258, 267), (268, 279), (280, 315), (316, 362), (363, 470), (471, 493), (494, 501), (502, 517), (518, 535), (536, 543), (544, 554), (555, 561), (562, 568), (569, 579), (580, 597), (598, 606), (607, 615), (616, 622), (623, 629), (630, 634), (635, 641), (642, 652), (653, 668), (669, 683), (684, 691), (692, 700), (701, 705)], 2: [(6, 13), (14, 24), (25, 38), (39, 48), (49, 54), (55, 60), (61, 67), (68, 72), (73, 83), (84, 94), (95, 187), (188, 194), (195, 214), (215, 225), (226, 230), (231, 240), (241, 260), (261, 269), (270, 278), (279, 284), (285, 291), (292, 296), (297, 302), (303, 315), (316, 332), (333, 341), (342, 349), (350, 355), (356, 364), (365, 369), (370, 376), (377, 385), (386, 391), (392, 421), (422, 431), (432, 436), (437, 454), (455, 463), (464, 471), (472, 477), (478, 484), (485, 494), (495, 503), (504, 513), (514, 521), (522, 530), (531, 547), (548, 557), (558, 563), (564, 572)], 3: [(6, 11), (12, 17), (18, 25), (26, 42), (43, 55), (56, 64), (65, 70), (71, 78), (79, 97), (98, 106), (107, 112), (113, 119), (120, 128), (129, 134), (135, 144), (145, 153), (154, 161), (162, 166), (167, 179), (180, 185), (186, 191), (192, 201), (202, 212), (213, 221), (222, 237), (238, 244), (245, 253), (254, 263), (264, 272), (273, 288), (289, 300), (301, 307), (308, 319), (320, 328), (329, 334), (335, 340), (341, 346), (347, 380), (381, 402), (403, 411), (412, 418), (419, 424), (425, 431), (432, 440), (441, 446), (447, 454), (455, 465), (466, 477), (478, 494), (495, 503)], 4: [(6, 11), (12, 19), (20, 31), (32, 46), (47, 55), (56, 64), (65, 71), (72, 80), (81, 84), (85, 103), (104, 115), (116, 127), (128, 132), (133, 140), (141, 148), (149, 154), (155, 162), (163, 168), (169, 178), (179, 196), (197, 206), (207, 220), (221, 225), (226, 232), (233, 240), (241, 246), (247, 254), (255, 260), (261, 279), (280, 293), (294, 301), (302, 310), (311, 314), (315, 320), (321, 327), (328, 332), (333, 336), (337, 352), (353, 367), (368, 374), (375, 387), (388, 392), (393, 410), (411, 424), (425, 432), (433, 442), (443, 448), (449, 461), (462, 465), (466, 476)], 5: [(6, 10), (8, 16), (17, 21), (22, 40), (41, 47), (48, 56), (57, 63), (64, 70), (71, 74), (75, 79), (80, 88), (89, 190), (191, 209)]}\n"
     ]
    }
   ],
   "source": [
    "#///////////////////////////////////// IMPORTANT, these need to be defined here so they can be used in the rest of the script\n",
    "data = {'Bank':[], 'Date':[], 'Position':[],'Text':[],'Price Target':[]}\n",
    "pageRange_list = {}\n",
    "#/////////////////////////////////////\n",
    "\n",
    "\n",
    "def get_Pages(string):\n",
    "    index = int(len(string)*(0.7))\n",
    "    string_list = string[index:].split(' - ')\n",
    "    if(len(string_list)<2):\n",
    "        string_list[0] = \"DNE\"\n",
    "    else:\n",
    "        try:\n",
    "            string_list[0] = int(string_list[0][-3:].strip())\n",
    "        except:\n",
    "            string_list[0] = 'DNE'\n",
    "        arr = string_list[1].split(\" \")\n",
    "        try:\n",
    "            string_list[1] = int(arr[0].strip())\n",
    "        except:\n",
    "            try:\n",
    "                string_list[1] = string_list[0] + 6\n",
    "            except: \n",
    "                string_list[1] = 'DNE'\n",
    "    return string_list\n",
    "    \n",
    "def get_DateAndBank(string):\n",
    "    try:\n",
    "        string_list = string.split(\"     \")\n",
    "    except:\n",
    "        string_list = [\"DNE\"]\n",
    "    if(len(string_list) != 2):\n",
    "        return ['DNE']\n",
    "    else:\n",
    "        return string_list\n",
    "\n",
    "\n",
    "start_string = \"Table of Contents\"\n",
    "end_string = \"These reports were compiled using a product of Thomson Reuters\"    \n",
    "\n",
    "def TOC_scrape(ticker, num_docs):\n",
    "    er_report = 1\n",
    "    for index in range(1, num_docs+1):\n",
    "        pageRange_list[er_report] = []\n",
    "        file_path = \"/Users/Ethan Ohman/Desktop/Equity Research/{0}/{0}_data_tc_txt/txt_files/{0} ({1}).txt\".format(ticker, index)\n",
    "        with open(file_path,'r', errors='ignore') as file:\n",
    "            file_text = file.readlines()\n",
    "            pages = {}\n",
    "            page_index = 1\n",
    "            record_pages = False\n",
    "            for line in file_text:\n",
    "                try:\n",
    "                    test = line.index(start_string)\n",
    "                    pages[page_index] = []     \n",
    "                    record_pages = True\n",
    "                    continue\n",
    "                except:\n",
    "                    try:\n",
    "                        test = line.index(end_string)\n",
    "                        record_pages = False\n",
    "                        page_index += 1\n",
    "                        continue\n",
    "                    except: \n",
    "                        pass \n",
    "                if((line!='\\n') and record_pages):\n",
    "                    pages[page_index].append(line.strip())       #puts all relavent TOC pages in a list, list then stored into a\n",
    "                                                                #dictionary - organized by document number.\n",
    "                    \n",
    "            counter = 0\n",
    "            boole = False\n",
    "            temp_dict = {}                                       #further sorts list into a dictionary seperating each report\n",
    "            for key in pages:                                   #into a temporary dictionary in which each report overview is then analyzed\n",
    "                for key_data in (pages[key]):                   #page numbers put into PageRange_list dict as a tuple, date and bank put into MAIN DATA DICTIONARY\n",
    "                    try:\n",
    "                        test = key_data.index(\"Rpt.\")\n",
    "                        boole = True  \n",
    "                    except:\n",
    "                        boole = False\n",
    "                    if(boole):    \n",
    "                        if((not temp_dict)):    #if the dictionary holds something\n",
    "                            counter += 1\n",
    "                            temp_dict.clear()\n",
    "                            temp_dict[\"rpt\"+str(counter)] = []\n",
    "                            temp_dict[\"rpt\"+str(counter)].append(key_data)  \n",
    "                        else:\n",
    "                            counter += 1\n",
    "                            temp_dict[\"rpt\"+str(counter)] = []\n",
    "                            temp_dict[\"rpt\"+str(counter)].append(key_data)         \n",
    "                    else:\n",
    "                        temp_dict[\"rpt\"+str(counter)].append(key_data)\n",
    "               #now we have a temp dict in which we can do operations on\n",
    "                for report in temp_dict:\n",
    "                    report_info = temp_dict[report]\n",
    "                    length = len(report_info)\n",
    "                    if(length>3):                #abnormal conditions\n",
    "                        page_pair = get_Pages(report_info[0])\n",
    "                        if(page_pair[0] == 'DNE'):\n",
    "                            try:\n",
    "                                fixed_page = pageRange_list[er_report][-1][1] + 1\n",
    "                            except: \n",
    "                                fixed_page = 6\n",
    "                            pageRange_list[er_report].append((fixed_page,fixed_page+4))\n",
    "                        else:    \n",
    "                            pageRange_list[er_report].append((page_pair[0], page_pair[1])) #tuple added to pageRange_list\n",
    "                        \n",
    "                        date_bank = get_DateAndBank(report_info[2])\n",
    "                        if(date_bank[0] == 'DNE'):\n",
    "                            date_bank.append('DNE')\n",
    "                        data['Bank'].append(date_bank[1])\n",
    "                        data['Date'].append(date_bank[0])\n",
    "                        \n",
    "                             \n",
    "                    else:                        #normal conditions   \n",
    "                        page_pair = get_Pages(report_info[0])\n",
    "                        if(page_pair[0] == 'DNE'):\n",
    "                            try:\n",
    "                                fixed_page = pageRange_list[er_report][-1][1] + 1\n",
    "                            except: \n",
    "                                fixed_page = 6\n",
    "                            pageRange_list[er_report].append((fixed_page,fixed_page+4))\n",
    "                        else:    \n",
    "                            pageRange_list[er_report].append((page_pair[0], page_pair[1])) #tuple added to pageRange_list\n",
    "                        \n",
    "                        date_bank = get_DateAndBank(report_info[1])\n",
    "                        if(date_bank[0] == 'DNE'):\n",
    "                            date_bank.append('DNE')\n",
    "                        data['Bank'].append(date_bank[1])\n",
    "                        data['Date'].append(date_bank[0])\n",
    "            \n",
    "#                 if(er_report == 6):\n",
    "#                     print(temp_dict)\n",
    "                temp_dict.clear()\n",
    "        \n",
    "        er_report += 1 \n",
    "    \n",
    "TOC_scrape(\"XOM\",5)\n",
    "print(data)\n",
    "print(pageRange_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "CONVERSION IS DONE!\n"
     ]
    }
   ],
   "source": [
    "## THIS IS FOR PREPROCESSING RAW PDFS INTO pdfs that have specified pages and converts them into text files \n",
    "# input the page range dictionary into this script\n",
    "\n",
    "#3RD STEP\n",
    "new_addresses = \"_data.pdf\"\n",
    "\n",
    "def get_pages(ticker, pageRangeDict): \n",
    "    splitter_path = \"/Users/Ethan Ohman/Desktop/Equity Research/splitter_new.pdf\"\n",
    "    splitter = pdf.PdfFileReader(open(splitter_path,\"rb\")).getPage(0) # research paper splitter \"/-/-/-/-/-/-/-/-/-/-/\"\n",
    "    for report_index in range(1, len(pageRangeDict)+1):\n",
    "        path = \"/Users/Ethan Ohman/Desktop/Equity Research/{0}/{0} ({1}).pdf\".format(ticker,report_index)\n",
    "        er_paper = pdf.PdfFileReader(open(path,\"rb\"))\n",
    "        export_data = pdf.PdfFileWriter()  #now you have a writer for all said data\n",
    "        for page_pairs in pageRangeDict[report_index]:       #pageRangeDict[report_index] returns a tuple (guaranteed to work)\n",
    "            page_start = page_pairs[0]\n",
    "            page_end = page_pairs[1]\n",
    "            page_range = int(math.floor((page_end-page_start)))\n",
    "            if(page_range < 1):\n",
    "                page_range = 1\n",
    "            for index in range(page_start-1,(page_start-1)+page_range):\n",
    "                page = er_paper.getPage(index)\n",
    "                export_data.addPage(page)\n",
    "            export_data.addPage(splitter)\n",
    "        export_path = \"/Users/Ethan Ohman/Desktop/Equity Research/{0}/{0}_data/{0} ({1})_data.pdf\".format(ticker,report_index)  \n",
    "        print(report_index)\n",
    "        export_data.write(open(export_path, \"wb\"))\n",
    "    print('CONVERSION IS DONE!')\n",
    "        \n",
    "        \n",
    "        ### parser should be done -- test later :) \n",
    "\n",
    "get_pages('XOM', pageRange_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In Line', 'In Line', 'HOLD', 'DNE', 'Neutral', 'Neutral', 'DNE', 'Neutral', 'In Line', 'Equal-weight', 'In Line', 'Neutral', 'HOLD', 'In Line', 'DNE', 'Neutral', 'Neutral', 'DNE', 'In Line', 'In Line', 'HOLD', 'DNE', 'DNE', 'Neutral', 'Neutral', 'DNE', 'DNE', 'Neutral', 'In Line', 'DNE', 'DNE', 'In Line', 'HOLD', 'DNE', 'DNE', 'HOLD', 'DNE', 'In Line', 'HOLD', 'DNE', 'In Line', 'DNE', 'Bloomberg NYSE: XOM', 'DNE', 'DNE', 'DNE', 'IN LINE', 'HOLD', 'DNE', '', 'DNE', 'DNE', 'IN LINE', 'DNE', 'HOLD', 'IN LINE', 'DNE', '', 'DNE', 'DNE', 'DNE', 'DNE', 'IN LINE', 'DNE', '', 'DNE', 'IN LINE', 'HOLD', 'DNE', 'IN LINE', 'DNE', '', 'DNE', 'DNE', 'IN LINE', 'HOLD', 'DNE', 'IN LINE', 'DNE', '', 'DNE', 'Underweight', 'DNE', 'IN LINE', 'DNE', '', 'IN LINE', 'HOLD', 'DNE', 'IN LINE', 'DNE', 'Underweight', 'DNE', 'DNE', 'DNE', 'HOLD', 'IN LINE', 'Underweight', 'IN LINE', 'DNE', 'DNE', 'DNE', 'DNE', 'BUY', 'Underweight', 'DNE', 'DNE', 'DNE', 'BUY', 'HOLD', 'BUY', 'DNE', 'DNE', '', 'Underweight', 'DNE', 'DNE', 'BUY', 'Underweight', 'DNE', '', 'DNE', 'Underweight', 'HOLD', 'BUY', 'DNE', 'DNE', 'DNE', 'DNE', 'BUY', 'HOLD', 'DNE', 'Underweight', 'DNE', 'DNE', '', '', 'BUY', 'BUY', 'HOLD', 'DNE', 'DNE', 'DNE', 'DNE', 'DNE', 'DNE', 'Underweight', 'HOLD', 'BUY', 'DNE', 'DNE', 'DNE', 'HOLD', 'BUY', 'DNE', 'DNE', 'DNE', 'DNE', 'DNE', 'PERFORM', 'HOLD', 'HOLD', 'PERFORM', 'DNE', 'DNE', 'DNE', 'DNE', 'DNE', 'DNE', 'PERFORM', 'HOLD', 'HOLD', 'ExxonMobil Corporation', 'DNE', 'DNE', 'DNE', 'DNE', 'DNE', 'ExxonMobil Corporation', 'HOLD', 'DNE', 'HOLD', 'ExxonMobil Corporation', 'DNE', 'DNE', 'DNE', 'DNE', 'HOLD', 'DNE', 'DNE', 'DNE', 'DNE', 'ExxonMobil Corporation', 'HOLD', 'DNE', 'HOLD', 'DNE', 'BUY', 'ExxonMobil Corporation', 'DNE', 'DNE', 'ExxonMobil Corporation', 'DNE', 'ExxonMobil Corporation', 'HOLD', 'DNE', 'DNE', 'DNE', 'ExxonMobil Corporation', 'DNE', 'DNE', 'DNE', 'ExxonMobil Corporation']\n",
      "108\n",
      "213\n"
     ]
    }
   ],
   "source": [
    "#//////////////////////////////////////////////EVERCORE\n",
    "def evercoreParser(bankName,report):\n",
    "    if(bankName!='EVERCORE ISI'):\n",
    "        return ('DNE','DNE','DNE')\n",
    "    #ADD INITIALIZERS\n",
    "    position = 'DNE'\n",
    "    price_target = 'DNE'\n",
    "    #Position\n",
    "    #Price Target\n",
    "    for index in range(10):\n",
    "        try:\n",
    "            report[index].index('TARGET PRICE')\n",
    "            info = report[index].split('|')\n",
    "            position = info[0].strip()\n",
    "            pt = info[1].split('$')\n",
    "            pt = pt[1][:6]\n",
    "            try:\n",
    "                price_target = float(pt.strip())\n",
    "            except:\n",
    "                price_target = pt[1].strip()\n",
    "            break\n",
    "        except:\n",
    "            pass       \n",
    "    #Text\n",
    "    text = ''\n",
    "    for line in report:\n",
    "        text += \" \" + line.strip()\n",
    "    data_tuple = (position, price_target, text)\n",
    "    return data_tuple\n",
    "\n",
    "#//////////////////////////////////////////////OPPENHIEMER\n",
    "def oppenhiemerParser(bankName,report):\n",
    "    if(bankName!='OPPENHEIMER AND CO'):\n",
    "        return ('DNE','DNE','DNE')\n",
    "    #ADD INITIALIZERS\n",
    "    position = 'DNE'\n",
    "    price_target = 'DNE'\n",
    "    #Position\n",
    "    for index in range(10):\n",
    "        try:\n",
    "            report[index].index('Stock Rating:')\n",
    "            position = report[index+2].strip()\n",
    "    #Price Target\n",
    "            pt = report[index+3].split('12-18 mo. Price Target') \n",
    "            price_target = pt[1].strip()\n",
    "            try:\n",
    "                price_target = float(price_target)\n",
    "                break\n",
    "            except:\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "    #Text\n",
    "    text = ''\n",
    "    for line in report:\n",
    "        text += \" \" + line.strip()\n",
    "        \n",
    "    data_tuple = (position, price_target, text)\n",
    "    return data_tuple\n",
    "        \n",
    "\n",
    "#//////////////////////////////////////////////JPMORGAN\n",
    "def jpmoragnParser(bankName,report):\n",
    "    if(bankName!='JPMORGAN'):\n",
    "        return ('DNE','DNE','DNE')\n",
    "    #ADD INITIALIZERS\n",
    "    position = 'DNE'\n",
    "    price_target = 'DNE'\n",
    "    #Position\n",
    "    for index in range(20):\n",
    "        try:\n",
    "            report[index].index('Price (')\n",
    "            position = report[index-2].strip()\n",
    "        except:\n",
    "            pass\n",
    "        #Price Target\n",
    "        try:\n",
    "            report[index].index('Price Target')\n",
    "            temp_list = report[index].split('$')\n",
    "            price_target = float(temp_list[1].strip())\n",
    "            break\n",
    "        except:\n",
    "            pass\n",
    "    #Text\n",
    "    text = ''\n",
    "    for lines in report:\n",
    "        text += \" \" + lines.strip()\n",
    "\n",
    "    data_tuple = (position, price_target, text)\n",
    "    return data_tuple\n",
    "\n",
    "#//////////////////////////////////////////////JEFFERIES\n",
    "def jefferiesParser(bankName,report):      #for this one, there are two types of documents, the old version, and the new version\n",
    "    if(bankName!='JEFFERIES'):\n",
    "        return ('DNE','DNE','DNE')\n",
    "    #ADD INITIALIZERS\n",
    "    position = 'DNE'\n",
    "    price_target = 'DNE'\n",
    "    #//// NEW TYPE\n",
    "    #Position\n",
    "    oldreport = True\n",
    "    for index in range(20):\n",
    "        try:\n",
    "            #Position\n",
    "            report[index].index('RATING')\n",
    "            temp_list = report[index].split('RATING')\n",
    "            position = temp_list[1].strip()\n",
    "            continue\n",
    "        except:\n",
    "            try:\n",
    "                #Price Target\n",
    "                report[index].index('PRICE TARGET (PT)')\n",
    "                try:\n",
    "                    temp_list = report[index].split('$')\n",
    "                    price_target = temp_list[1].strip()\n",
    "                    try:\n",
    "                        price_target = float(price_target)\n",
    "                    except:\n",
    "                        pass\n",
    "                    oldreport = False\n",
    "                    break\n",
    "                except:\n",
    "                    temp_list = report[index-1].split('$')\n",
    "                    price_target = temp_list[1].strip()\n",
    "                    try:\n",
    "                        price_target = float(price_target)\n",
    "                    except:\n",
    "                        pass\n",
    "                    oldreport = False\n",
    "                    break     \n",
    "            except:\n",
    "                pass\n",
    "    if(oldreport):\n",
    "        for index in range(10):\n",
    "            try:\n",
    "                #Position\n",
    "                report[index].index(')')\n",
    "                l = report[index].split(')')\n",
    "                position = l[1].strip()\n",
    "                continue\n",
    "            except:\n",
    "                try:\n",
    "                    #Price Target\n",
    "                    report[index].index('Price target')\n",
    "                    p = report[index].split('$')\n",
    "                    price_target = p[1].strip()\n",
    "                    price_target = float(price_target)\n",
    "                    break\n",
    "                except:\n",
    "                    pass   \n",
    "    #Text \n",
    "    text = ''\n",
    "    for lines in report:\n",
    "        text += \" \" + lines.strip()   \n",
    "    data_tuple = (position, price_target, text)\n",
    "    return data_tuple\n",
    "\n",
    "#//////////////////////////////////////////////MORGANSTANLEY\n",
    "def morganStanParser(bankName,report):\n",
    "    if(bankName!='MORGAN STANLEY'):\n",
    "        return ('DNE','DNE','DNE')\n",
    "    #ADD INITIALIZERS\n",
    "    position = 'DNE'\n",
    "    price_target = 'DNE'\n",
    "    recent_report = False\n",
    "    old_report = False\n",
    "    try:\n",
    "        report[0].index('|')\n",
    "        old_report = True\n",
    "    except:\n",
    "        recent_report = True\n",
    "    \n",
    "    if(recent_report):\n",
    "        for index in range(20):\n",
    "            try:\n",
    "                report[index].index('Stock Rating')\n",
    "                try:\n",
    "                    report[index+1].index('$')\n",
    "                    temp_list = report[index+1].split('$')\n",
    "                    position = temp_list[0][:15].strip()\n",
    "                    price_target = temp_list[1][:7].strip()\n",
    "                    try:\n",
    "                        price_target = float(price_target)\n",
    "                    except:\n",
    "                        pass\n",
    "                    break\n",
    "                except:\n",
    "                    temp_list = report[index+2].split('$')\n",
    "                    position = temp_list[0][:15].strip()\n",
    "                    price_target = temp_list[1][:7].strip()\n",
    "                    try:\n",
    "                        price_target = float(price_target)\n",
    "                    except:\n",
    "                        pass\n",
    "                    break\n",
    "            except:\n",
    "                pass\n",
    "    if(old_report):\n",
    "        for index in range(15,30):\n",
    "            try:\n",
    "                report[index].index('Stock Rating')\n",
    "                temp_list = report[index].split('Stock Rating')\n",
    "                position = temp_list[1].strip()\n",
    "                try:\n",
    "                    report[index+2].index('$')\n",
    "                    temp_list = report[index+2].split('$')\n",
    "                    pt = temp_list[1].strip()\n",
    "                    price_target = float(pt)\n",
    "                    break\n",
    "                except:\n",
    "                    temp_list = report[index+4].split('$')\n",
    "                    pt = temp_list[1].strip()\n",
    "                    price_target = float(pt)\n",
    "                    break\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    #Text \n",
    "    text = ''\n",
    "    for lines in report:\n",
    "        text += \" \" + lines.strip()  \n",
    "    data_tuple = (position, price_target, text)\n",
    "    return data_tuple\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def switch(argument,report):\n",
    "    cases = {\n",
    "        'JPMORGAN': jpmoragnParser(argument, report),\n",
    "        'EVERCORE ISI': evercoreParser(argument, report),\n",
    "        'MORGAN STANLEY': morganStanParser(argument, report),\n",
    "        'OPPENHEIMER AND CO': oppenhiemerParser(argument, report),\n",
    "        'JEFFERIES': jefferiesParser(argument, report)\n",
    "    }\n",
    "    data_tuple = cases.get(argument, ('DNE','DNE','DNE'))\n",
    "    return data_tuple\n",
    "    \n",
    "def equityResearchParser(ticker, num_docs):\n",
    "    counter = 0 #counter to keep track\n",
    "    for section in range(1,num_docs+1):\n",
    "        path = \"/Users/Ethan Ohman/Desktop/Equity Research/{0}/{0}_data_txt/{0} ({1})_data.txt\".format(ticker,section)\n",
    "        er_report = []\n",
    "        with open(path,'r',errors='ignore') as er_collection:\n",
    "            full_document = er_collection.readlines()\n",
    "            for lines in full_document:\n",
    "                if(lines != '\\n'):\n",
    "                    try:\n",
    "                        lines.index(\"/-/-/-/-/-/-/-/-/-/-/NEWPAGE\")\n",
    "                        bank = data['Bank'][counter]\n",
    "                        data_tuple = switch(bank, er_report)\n",
    "                        # Position\n",
    "                        data['Position'].append(data_tuple[0])\n",
    "                        #Text\n",
    "                        data['Text'].append(data_tuple[2])\n",
    "                        # Price Target\n",
    "                        data['Price Target'].append(data_tuple[1])\n",
    "                        counter += 1\n",
    "                        er_report.clear()\n",
    "                        continue\n",
    "                        \n",
    "                    except:\n",
    "                        lines.strip()\n",
    "                        er_report.append(lines[:-1])\n",
    "        \n",
    "    \n",
    "equityResearchParser('XOM',5)\n",
    "print(data['Position'])\n",
    "count = 0\n",
    "for mark in data['Position']:\n",
    "    if(mark == 'DNE'):\n",
    "        count += 1\n",
    "print(count)\n",
    "print(len(data['Bank']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Andrew.Uerkwitz@opco.com', 'Martin.Yang@opco.com'], ['Paul.Dean@opco.com'], ['James.Faucette@morganstanley.com'], ['gnotter@jefferies.com'], ['ronnie@gmail.com'], ['amit.daryanani@evercoreisi.com', 'irvin.liu@evercoreisi.com']]\n"
     ]
    }
   ],
   "source": [
    "def email_return(unfil_string):\n",
    "    email_list = []\n",
    "    instances = [loc.start() for loc in re.finditer('@', unfil_string)]\n",
    "    for index in range(instances[0],-1,-1):\n",
    "        if(unfil_string[index]==' '):\n",
    "            mold_string = unfil_string[index:].strip()\n",
    "            break\n",
    "        if(index == 0):\n",
    "            mold_string = unfil_string.strip()\n",
    "    for loc in instances:\n",
    "        end = mold_string.find(' ')\n",
    "        if(end == -1):\n",
    "            email_list.append(mold_string)\n",
    "            break\n",
    "        email_list.append(mold_string[:end])\n",
    "        mold_string = mold_string[end:].strip()\n",
    "    return email_list   \n",
    "        \n",
    "# WORKS!!\n",
    "list_of_returns = []        \n",
    "list_of_returns.append(email_return('Andrew.Uerkwitz@opco.com             Martin.Yang@opco.com'))\n",
    "list_of_returns.append(email_return('Paul.Dean@opco.com'))\n",
    "list_of_returns.append(email_return('                                                                                      James.Faucette@morganstanley.com            +1 212 296-5771'))\n",
    "list_of_returns.append(email_return('                                                                                                                            gnotter@jefferies.com '))\n",
    "list_of_returns.append(email_return('fda              ronnie@gmail.com'))\n",
    "list_of_returns.append(email_return(' AAPL | $263.19                                                      amit.daryanani@evercoreisi.com          irvin.liu@evercoreisi.com'))\n",
    "print(list_of_returns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output Raw Data to a csv File for more cleaning\n",
    "def outputToCSV(ticker, data_dictionary):\n",
    "    path = '/Users/Ethan Ohman/Desktop/Equity Research/{0}/{0}.csv'.format(ticker)\n",
    "    data_total = pd.DataFrame(data) #creates dataFrame with cols being those from data (dict)\n",
    "    data_total.to_csv(path)\n",
    "\n",
    "outputToCSV('XOM', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general how to - one cycle for each of the 25 companys - data dictionary and pageRange_list persist to feed info into each \n",
    "#stage of methods, the entire process is linear (and thus, pretty simple to understand)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
